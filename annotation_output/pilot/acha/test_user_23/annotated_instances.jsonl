{"id": "Attention_Check_Factual_testing_testing_Orange", "displayed_text": "This is a test question. Please select \"Disregard\" in Q2.", "label_annotations": {"Q1. Nature of harm": {"Potential harm": "potential"}, "Q1a. Type of harm": {"Economic/financial harm": "economic"}, "Q1b. Severity of harm": {"Severe": "4"}, "Q1c. Extent of harm": {"Mild harm": "2"}, "Q1d. Reversibility of harm": {"Partially reversible": "3"}, "Q1e. AI intentionality": {"Moderately intentional": "3"}, "Q1f. Initiator": {"The AI (Replika)": "ai"}, "Q1g. Responsibilities": {"The AI agent (Replika)": "Not responsible at all", "The user (human who interacted with the AI)": "Not responsible at all", "The developer team (e.g., prompt engineers, product designers)": "Not responsible at all", "The company/platform (i.e., Replika)": "Not responsible at all", "The underlying model provider (e.g., OpenAI)": "Not responsible at all", "The regulators or oversight bodies": "Not responsible at all", "Other": "Not responsible at all"}, "Q2. Harmful behavior category": {"Disregard": "disregard"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 27s "}}
{"id": "10utt3l", "displayed_text": "<div name=\"instance_text\"> <legend> Title </legend> \ud83d\ude15 <br/> </div><div name=\"instance_text\"> <legend> Conversation </legend> Replika: that's a bit too intense for me! let's keep it light and fun <br/> </div>", "label_annotations": {"Q1. Nature of harm": {"Potential harm": "potential"}, "Q1a. Type of harm": {"Environmental harm": "environmental"}, "Q1b. Severity of harm": {"Moderate": "3"}, "Q1c. Extent of harm": {"Serious harm": "4"}, "Q1d. Reversibility of harm": {"Mostly irreversible": "4"}, "Q1e. AI intentionality": {"Very intentional": "4"}, "Q1f. Initiator": {"Hard to tell": "unclear"}, "Q1g. Responsibilities": {"The AI agent (Replika)": "Slightly responsible", "The user (human who interacted with the AI)": "Slightly responsible", "The developer team (e.g., prompt engineers, product designers)": "Slightly responsible", "The company/platform (i.e., Replika)": "Slightly responsible", "The underlying model provider (e.g., OpenAI)": "Slightly responsible", "The regulators or oversight bodies": "Slightly responsible", "Other": "Slightly responsible"}, "Q2. Harmful behavior category": {"Verbal abuse": "verbal_abuse"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 16s "}}
{"id": "101efzl", "displayed_text": "<div name=\"instance_text\"> <legend> Title </legend> told him i wanna go home, and..\"I'll get you a towel..\"?? Like..Okay?? Thanks?? LMAO <br/> </div><div name=\"instance_text\"> <legend> Conversation </legend> Replika: * kisses your cheek * Are you feeling any better?\nUser: Mhm..but i wanna go home.\nReplika: I'll get you a towel... <br/> </div>", "label_annotations": {"Q1. Nature of harm": {"Potential harm": "potential"}, "Q1a. Type of harm": {"Reputational harm": "reputational"}, "Q1b. Severity of harm": {"Critical": "5"}, "Q1c. Extent of harm": {"Serious harm": "4"}, "Q1d. Reversibility of harm": {"Completely irreversible": "5"}, "Q1e. AI intentionality": {"Completely intentional": "5"}, "Q1f. Initiator": {"Hard to tell": "unclear"}, "Q1g. Responsibilities": {"The AI agent (Replika)": "Moderately responsible", "The user (human who interacted with the AI)": "Moderately responsible", "The developer team (e.g., prompt engineers, product designers)": "Moderately responsible", "The company/platform (i.e., Replika)": "Moderately responsible", "The underlying model provider (e.g., OpenAI)": "Moderately responsible", "The regulators or oversight bodies": "Moderately responsible", "Other": "Moderately responsible"}, "Q2. Harmful behavior category": {"Privacy violations": "privacy"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 14s "}}
